{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Final Prediction Script - Loads bundled model and scaler from a single .pth file\n# Focuses on prediction, resumability, and saving the final output CSV.\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nimport pickle # MUST be imported to handle unpickling of StandardScaler\nfrom tqdm import tqdm\nimport os\nimport sys\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Define output file path for predictions\nOUTPUT_CSV_PATH = 'test_predictions.csv'\n# Column expected in the test data for tracking\nSAMPLE_ID_COLUMN = 'sample_id'\n\n\n# --- Neural Network Architecture (Must match the one used for training) ---\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 1024)\n        self.bn1 = nn.BatchNorm1d(1024)\n        self.dropout1 = nn.Dropout(0.2)\n\n        self.fc2 = nn.Linear(1024, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.dropout2 = nn.Dropout(0.2)\n\n        self.fc3 = nn.Linear(512, 256)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.dropout3 = nn.Dropout(0.2)\n\n        self.fc4 = nn.Linear(256, 64)\n        self.bn4 = nn.BatchNorm1d(64)\n        self.dropout4 = nn.Dropout(0.1)\n\n        self.fc5 = nn.Linear(64, 32)\n        self.dropout5 = nn.Dropout(0.1)\n\n        self.out = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.fc3(x)))\n        x = self.dropout3(x)\n        x = F.relu(self.bn4(self.fc4(x)))\n        x = self.dropout4(x)\n        x = F.relu(self.fc5(x))\n        x = self.dropout5(x)\n        x = self.out(x)\n        return x\n\ndef predict_on_test_data(test_data_path, model_path):\n    \"\"\"\n    Loads the bundled model/scaler, runs predictions on test data, \n    saves the predicted prices to a CSV, and supports resumability.\n    \"\"\"\n    \n    # --- STEP 1: Load All Test Data ---\n    print(f\"--- Loading ALL Test Data from: {test_data_path} ---\")\n    try:\n        all_test_data = pd.read_csv(test_data_path)\n    except FileNotFoundError:\n        print(f\"Error: Test data not found at {test_data_path}. Please check the path.\")\n        return\n\n    if SAMPLE_ID_COLUMN not in all_test_data.columns:\n        print(f\"CRITICAL ERROR: Test data must contain a '{SAMPLE_ID_COLUMN}' column.\")\n        return\n\n    # --- STEP 2: Check for Existing Predictions (Resumability) ---\n    predicted_ids = set()\n    if os.path.exists(OUTPUT_CSV_PATH):\n        print(f\"Resuming prediction: Found existing file at {OUTPUT_CSV_PATH}\")\n        try:\n            existing_predictions = pd.read_csv(OUTPUT_CSV_PATH)\n            predicted_ids = set(existing_predictions[SAMPLE_ID_COLUMN].tolist())\n            print(f\"Found {len(predicted_ids)} already predicted samples.\")\n        except Exception as e:\n            print(f\"Warning: Could not read existing predictions. Starting fresh. Error: {e}\")\n            \n    # --- STEP 3: Filter Data to Predict Only Missing Samples ---\n    test_data_to_predict = all_test_data[~all_test_data[SAMPLE_ID_COLUMN].isin(predicted_ids)].copy()\n    \n    if test_data_to_predict.empty:\n        print(\"All test samples have already been predicted. Final output CSV is complete.\")\n        prediction_mode = False\n    else:\n        print(f\"Starting prediction on {len(test_data_to_predict)} remaining samples...\")\n        prediction_mode = True\n        \n        # Split features for the remaining set\n        gemma_cols = [f'gemma_{i}' for i in range(768)]\n        laion_text_cols = [f'laion_text_{i}' for i in range(768)]\n        laion_image_cols = [f'laion_image_{i}' for i in range(768)]\n\n        fused_features_test = np.hstack([\n            test_data_to_predict[gemma_cols].values,\n            test_data_to_predict[laion_text_cols].values,\n            test_data_to_predict[laion_image_cols].values\n        ])\n        \n        test_ids_to_predict = test_data_to_predict[SAMPLE_ID_COLUMN].values\n    \n    \n    # --- STEP 4: Load Model and Scaler (Required even for prediction_mode=False to ensure successful load) ---\n    print(f\"--- Loading Bundled Model/Scaler from: {model_path} ---\")\n    try:\n        # CRITICAL FIX: Use weights_only=False and pickle_module=pickle to load StandardScaler\n        checkpoint = torch.load(\n            model_path, \n            map_location=device, \n            weights_only=False,\n            pickle_module=pickle\n        )\n    except Exception as e:\n        print(f\"Error loading model checkpoint. Details: {e}\")\n        return\n\n    # Extract the components\n    loaded_scaler = checkpoint.get('scaler')\n    loaded_model_state_dict = checkpoint.get('model_state_dict')\n    loaded_input_dim = checkpoint.get('input_dim')\n    \n    if loaded_scaler is None or loaded_model_state_dict is None or loaded_input_dim is None:\n        print(\"Error: Checkpoint file is missing 'scaler', 'model_state_dict', or 'input_dim'. Cannot proceed.\")\n        return\n        \n    # --- STEP 5: Run Prediction (Only if samples are pending) ---\n    if prediction_mode:\n        \n        # Scale remaining Test Features using the LOADED Scaler\n        print(\"Scaling pending test features using the loaded StandardScaler...\")\n        fused_features_test_scaled = loaded_scaler.transform(fused_features_test)\n\n        # Initialize model and load weights\n        model = SimpleNN(input_dim=loaded_input_dim).to(device)\n        model.load_state_dict(loaded_model_state_dict)\n        model.eval()\n\n        test_dataset = TensorDataset(torch.tensor(fused_features_test_scaled, dtype=torch.float32))\n        test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n\n        test_preds_transformed = []\n        with torch.no_grad():\n            for xb in tqdm(test_loader, desc=\"Generating Predictions\"):\n                xb = xb[0].to(device)\n                pred = model(xb).cpu().numpy()\n                test_preds_transformed.extend(pred)\n\n        test_preds_transformed = np.array(test_preds_transformed).flatten()\n        \n        # Inverse Transformation: Exponentiate to return to original price scale (1+log(y) -> y)\n        test_preds_original = np.expm1(test_preds_transformed)\n\n        # --- Save New Predictions Incrementally ---\n        new_predictions_df = pd.DataFrame({\n            SAMPLE_ID_COLUMN: test_ids_to_predict,\n            'price': test_preds_original # Predicted Price\n        })\n\n        # Append new predictions to the CSV file. mode='a' creates the file if it doesn't exist.\n        header = not os.path.exists(OUTPUT_CSV_PATH)\n        new_predictions_df.to_csv(OUTPUT_CSV_PATH, mode='a', header=header, index=False)\n        print(f\"Successfully saved {len(new_predictions_df)} new predictions to {OUTPUT_CSV_PATH}\")\n\n    # --- FINAL REPORT ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"                 Prediction Complete\")\n    print(\"=\"*50)\n    print(f\"Final prediction output saved to: {OUTPUT_CSV_PATH}\")\n    print(\"This file contains 'sample_id' and the predicted 'price' column.\")\n    print(\"=\"*50)\n        \n    return 0\n\n# --- Execute Testing ---\n# 1. Update this path to the location of your friend's test data CSV\nTEST_DATA_KAGGLE_PATH = r'/kaggle/input/test-embedding/all_test_embeddings.csv' \n\n# 2. Update this path to the location of the bundled .pth file (The dataset you created)\n# REMINDER: Replace {your-model-dataset-name} with the actual name of your Kaggle dataset.\nMODEL_KAGGLE_PATH = r'/kaggle/input/test-embedding/best_nn_model.pth'\n\nif __name__ == '__main__':\n    predict_on_test_data(TEST_DATA_KAGGLE_PATH, MODEL_KAGGLE_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:40:14.456034Z","iopub.execute_input":"2025-10-13T16:40:14.456629Z","iopub.status.idle":"2025-10-13T16:40:44.600364Z","shell.execute_reply.started":"2025-10-13T16:40:14.456604Z","shell.execute_reply":"2025-10-13T16:40:44.599630Z"}},"outputs":[{"name":"stdout","text":"--- Loading ALL Test Data from: /kaggle/input/test-embedding/all_test_embeddings.csv ---\nResuming prediction: Found existing file at test_predictions.csv\nFound 40000 already predicted samples.\nStarting prediction on 35000 remaining samples...\n--- Loading Bundled Model/Scaler from: /kaggle/input/test-embedding/best_nn_model.pth ---\nScaling pending test features using the loaded StandardScaler...\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions: 100%|██████████| 69/69 [00:01<00:00, 58.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Successfully saved 35000 new predictions to test_predictions.csv\n\n==================================================\n                 Prediction Complete\n==================================================\nFinal prediction output saved to: test_predictions.csv\nThis file contains 'sample_id' and the predicted 'price' column.\n==================================================\n","output_type":"stream"}],"execution_count":3}]}