{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13352634,"sourceType":"datasetVersion","datasetId":8468710}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade libraft-cu12==25.6.*\n!pip install --upgrade pylibraft-cu12==25.6.*\n!pip install --upgrade rmm-cu12==25.6.*\n!pip install --upgrade cuml-cu12==25.6.*\n!pip install --upgrade raft-dask-cu12==25.6.*\n!pip install --upgrade cudf-cu12==25.6.*\n!pip install --upgrade cuvs-cu12==25.6.*\n!pip install --upgrade pylibcugraph-cu12==25.6.*\n\n!pip install pyarrow>=21.0.0\n!pip install \"pydantic<2.12,>=2.0\"\n!pip install dask==2024.12.1\n!pip install scikit-learn<1.6.0,>=1.0.0\n!pip install dask==2025.5.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:34:53.30946Z","iopub.execute_input":"2025-10-12T15:34:53.310301Z","iopub.status.idle":"2025-10-12T15:35:36.883091Z","shell.execute_reply.started":"2025-10-12T15:34:53.310271Z","shell.execute_reply":"2025-10-12T15:35:36.882305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!pip install sentence-transformers open-clip-torch transformers pandas pillow beautifulsoup4 tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:35:51.946001Z","iopub.execute_input":"2025-10-12T15:35:51.946709Z","iopub.status.idle":"2025-10-12T15:35:55.415198Z","shell.execute_reply.started":"2025-10-12T15:35:51.946679Z","shell.execute_reply":"2025-10-12T15:35:55.414501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\nfrom sentence_transformers import SentenceTransformer\nimport open_clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:36:18.62689Z","iopub.execute_input":"2025-10-12T15:36:18.627413Z","iopub.status.idle":"2025-10-12T15:36:18.631819Z","shell.execute_reply.started":"2025-10-12T15:36:18.627387Z","shell.execute_reply":"2025-10-12T15:36:18.631146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:36:37.480265Z","iopub.execute_input":"2025-10-12T15:36:37.480843Z","iopub.status.idle":"2025-10-12T15:36:37.485218Z","shell.execute_reply.started":"2025-10-12T15:36:37.480813Z","shell.execute_reply":"2025-10-12T15:36:37.484291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- CONFIG ----\n\nIMAGE_FOLDER = r\"/kaggle/input/amazon-images/images/images\"\nDATA_CSV_PATH = r\"/kaggle/input/amazon-images/train.csv\"\nGEMMA_MODEL_NAME = \"google/embeddinggemma-300m\"\nGEMMA_MAX_TOKENS = 2048\nLAION_MODEL_ID = 'hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K'\nLAION_TOKEN_LIMIT = 77\nBATCH_SIZE = 32\nOUTPUT_CSV_PATH = \"all_embeddings.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:36:41.658563Z","iopub.execute_input":"2025-10-12T15:36:41.658963Z","iopub.status.idle":"2025-10-12T15:36:41.662976Z","shell.execute_reply.started":"2025-10-12T15:36:41.658935Z","shell.execute_reply":"2025-10-12T15:36:41.662252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- PREPROCESSING FUNCTIONS ---\n\ndef is_html(text):\n    return bool(re.search(r'<[^>]+>', str(text)))\n\ndef clean_text_for_laion(text):\n    '''Cleaner for LAION OpenCLIP text embedding.'''\n    text = str(text).encode('utf-8', 'ignore').decode(errors='ignore')\n    if is_html(text):\n        text = BeautifulSoup(text, 'lxml').get_text()\n    # Remove emojis & corrupted unicode\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\ndef clean_text_for_gemma(text):\n    '''Cleaner for Gemma embedding input.'''\n    text = str(text).lower()\n    if is_html(text):\n        text = BeautifulSoup(text, 'lxml').get_text()\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ascii for simplicity\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:36:42.529247Z","iopub.execute_input":"2025-10-12T15:36:42.530014Z","iopub.status.idle":"2025-10-12T15:36:42.536056Z","shell.execute_reply.started":"2025-10-12T15:36:42.529988Z","shell.execute_reply":"2025-10-12T15:36:42.535204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Gemma SentenceTransformer Chunking ---\n\ndef chunk_text_for_sentence_transformer(text, tokenizer, max_tokens):\n    tokens = tokenizer.tokenize(text)\n    chunks, i = [], 0\n    while i < len(tokens):\n        chunk = tokens[i:i+max_tokens]\n        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n        chunks.append(chunk_text)\n        i += max_tokens\n    return chunks\n\ndef get_gemma_embedding(text, gemma_model):\n    tokenizer = gemma_model.tokenizer\n    chunks = chunk_text_for_sentence_transformer(text, tokenizer, GEMMA_MAX_TOKENS)\n    chunk_embeds = gemma_model.encode(chunks, device=DEVICE)\n    # Average the chunk embeddings\n    avg_embedding = torch.tensor(chunk_embeds).float().mean(dim=0).numpy()\n    return avg_embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:36:43.01259Z","iopub.execute_input":"2025-10-12T15:36:43.013201Z","iopub.status.idle":"2025-10-12T15:36:43.018116Z","shell.execute_reply.started":"2025-10-12T15:36:43.013179Z","shell.execute_reply":"2025-10-12T15:36:43.01738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  ----------------------\n\ndef get_laion_text_embedding_batch(texts, tokenizer, laion_model):\n    tokenized = []\n    for text in texts:\n        tokens = list(tokenizer.encode(text))\n        tokens = tokens[:LAION_TOKEN_LIMIT]\n\n        pad_len = LAION_TOKEN_LIMIT - len(tokens)\n        if pad_len > 0:\n            pad_token = tokenizer.eos_token_id if hasattr(tokenizer, \"eos_token_id\") else 0\n            tokens.extend([pad_token] * pad_len)\n\n        print(f\"Original length: {len(tokens) - pad_len}, Padded length: {len(tokens)}\")\n        tokenized.append(tokens)\n\n    assert all(len(t) == LAION_TOKEN_LIMIT for t in tokenized), \"Token length mismatch.\"\n\n        \n    text_tensor = torch.tensor(tokenized, dtype=torch.long).to(DEVICE)\n    with torch.no_grad():\n        emb = laion_model.encode_text(text_tensor)\n        emb = emb / emb.norm(dim=-1, keepdim=True)\n    return emb.cpu().numpy()\n\n\ndef get_laion_image_embedding_batch(image_paths, laion_model, image_preprocess):\n    images = []\n    for path in image_paths:\n        if os.path.exists(path):\n            try:\n                image = Image.open(path).convert(\"RGB\")\n                images.append(image_preprocess(image))\n            except Exception as e:\n                print(f\"Error loading image {path}: {e}\")\n                images.append(torch.zeros(3, 224, 224))\n        else:\n            images.append(torch.zeros(3, 224, 224))\n\n    image_tensor = torch.stack(images).to(DEVICE)\n    with torch.no_grad():\n        emb = laion_model.encode_image(image_tensor)\n        emb = emb / emb.norm(dim=-1, keepdim=True)\n    return emb.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:36:43.64497Z","iopub.execute_input":"2025-10-12T15:36:43.645623Z","iopub.status.idle":"2025-10-12T15:36:43.653033Z","shell.execute_reply.started":"2025-10-12T15:36:43.6456Z","shell.execute_reply":"2025-10-12T15:36:43.652373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-----------------------------\n\ndef main():\n    # Load models\n    gemma_model = SentenceTransformer(GEMMA_MODEL_NAME, device=DEVICE)\n    laion_model, image_preprocess_train, _ = open_clip.create_model_and_transforms(LAION_MODEL_ID, device=DEVICE)\n    laion_tokenizer = open_clip.get_tokenizer(LAION_MODEL_ID)\n\n    # Load CSV data\n    df = pd.read_csv(DATA_CSV_PATH)\n\n    # Load already processed sample_ids if output CSV exists\n    processed_ids = set()\n    if os.path.exists(OUTPUT_CSV_PATH):\n        processed_df = pd.read_csv(OUTPUT_CSV_PATH, usecols=[\"sample_id\"])\n        processed_ids = set(processed_df[\"sample_id\"].astype(str).tolist())\n        print(f\"Resuming from {len(processed_ids)} already processed samples.\")\n\n    output_rows = []\n    batch_sample_ids = []\n    batch_prices = []\n    batch_texts_gemma = []\n    batch_texts_laion = []\n    batch_image_paths = []\n\n\n\n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n\n        sample_id_str = str(row[\"sample_id\"])\n        if sample_id_str in processed_ids:\n            continue\n        \n        batch_sample_ids.append(row[\"sample_id\"])\n        batch_prices.append(row.get(\"price\", None))\n        batch_texts_gemma.append(clean_text_for_gemma(row[\"catalog_content\"]))\n        batch_texts_laion.append(clean_text_for_laion(row[\"catalog_content\"]))\n        image_filename = str(row[\"sample_id\"]) + \".jpg\"\n        batch_image_paths.append(os.path.join(IMAGE_FOLDER, image_filename))\n\n\n        # When batch full or last row\n        if len(batch_sample_ids) == BATCH_SIZE or idx == len(df) - 1:\n            # Gemma embeddings batch\n            gemma_emb_batch = []\n            for text in batch_texts_gemma:\n                gemma_emb_batch.append(get_gemma_embedding(text, gemma_model))\n            gemma_emb_batch = torch.tensor(gemma_emb_batch).numpy()\n\n            # LAION text embedding batch\n            laion_text_emb_batch = get_laion_text_embedding_batch(batch_texts_laion, laion_tokenizer, laion_model)\n\n            # LAION image embedding batch\n            laion_image_emb_batch = get_laion_image_embedding_batch(batch_image_paths, laion_model, image_preprocess_train)\n\n            # Compose output\n            for i in range(len(batch_sample_ids)):\n                out_row = {\n                    \"sample_id\": batch_sample_ids[i],\n                    \"price\": batch_prices[i],\n                    **{f\"gemma_{j}\": gemma_emb_batch[i, j] for j in range(gemma_emb_batch.shape[1])},\n                    **{f\"laion_text_{j}\": laion_text_emb_batch[i, j] for j in range(laion_text_emb_batch.shape[1])},\n                    **{f\"laion_image_{j}\": laion_image_emb_batch[i, j] for j in range(laion_image_emb_batch.shape[1])}\n                }\n                output_rows.append(out_row)\n\n\n            # Append batch results to CSV (create if not exists)\n            batch_df = pd.DataFrame(output_rows)\n            if not os.path.exists(OUTPUT_CSV_PATH):\n                batch_df.to_csv(OUTPUT_CSV_PATH, index=False)\n            else:\n                batch_df.to_csv(OUTPUT_CSV_PATH, mode='a', index=False, header=False)\n            print(f\"Saved {len(output_rows)} rows to {OUTPUT_CSV_PATH}\")\n\n            # Clear batches\n            \n            # Clear batches\n            output_rows = []\n            batch_sample_ids = []\n            batch_prices = []\n            batch_texts_gemma = []\n            batch_texts_laion = []\n            batch_image_paths = []\n\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:42:50.349397Z","iopub.execute_input":"2025-10-12T15:42:50.349896Z","iopub.status.idle":"2025-10-12T15:42:50.569604Z","shell.execute_reply.started":"2025-10-12T15:42:50.349874Z","shell.execute_reply":"2025-10-12T15:42:50.568481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}